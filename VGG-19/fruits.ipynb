{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data source = \"https://www.kaggle.com/datasets/mbkinaci/fruit-images-for-object-detection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"./fruits/train_zip/train/\"\n",
    "test_dir = \"./fruits/test_zip/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fruit_dir = \n",
    "train_imgs = os.listdir(\"fruits/train_zip/train\")\n",
    "test_imgs = os.listdir(\"fruits/test_zip/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i in train_imgs:\n",
    "    label = i.split('_')[0]\n",
    "    if label in labels:\n",
    "        pass\n",
    "    else:\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['orange', 'banana', 'mixed', 'apple']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 100\n",
    "\n",
    "def get_data(imgs_path):\n",
    "    data = []\n",
    "    for img in os.listdir(imgs_path):\n",
    "        im_s = img.split('.')[1]\n",
    "        if (im_s == 'jpg') or (im_s == 'jpeg') or (im_s == 'png'):\n",
    "            label = img.split('_')[0]\n",
    "            label_index = labels.index(label)\n",
    "            im_path = imgs_path+img\n",
    "            img_arr = cv2.imread(im_path, cv2.IMREAD_COLOR)\n",
    "            resize_img = cv2.resize(img_arr, (image_size, image_size))\n",
    "            data.append([resize_img, label_index])\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "/tmp/ipykernel_6496/407720218.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(data)\n"
     ]
    }
   ],
   "source": [
    "training_data = get_data(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "/tmp/ipykernel_6496/407720218.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(data)\n"
     ]
    }
   ],
   "source": [
    "test_data = get_data(test_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SPLIT DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_and_y(data):\n",
    "    X, y = [], []\n",
    "    for feature, label in data:\n",
    "        X.append(feature)\n",
    "        y.append(label)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_x_and_y(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = get_x_and_y(test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NORMALIZE DATA**\n",
    "\n",
    "Normalize X to form 0..1 for easier convergence by CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    return np.array(X) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESHAPE**\n",
    "\n",
    "Reshape data to 3-D from 1-D as required by CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(X, y):\n",
    "    lb = LabelBinarizer()\n",
    "    _x = X.reshape(-1, image_size, image_size, 3) # reshape to 3-d\n",
    "    _y = lb.fit_transform(y)\n",
    "    return _x, _y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = reshape_data(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = reshape_data(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL\n",
    "\n",
    "**Training VGG-19 Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 13:37:22.079641: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/charles/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-01-13 13:37:22.079676: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet import ResNet\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (image_size, image_size, 3)\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "def fruits_model():\n",
    "    model_trained = VGG19(input_shape=INPUT_SHAPE, include_top=False, weights='imagenet')\n",
    "    for layer in model_trained.layers[:19]:\n",
    "        layer.trainable = False\n",
    "\n",
    "        model = Sequential(\n",
    "            [\n",
    "                model_trained,\n",
    "                MaxPool2D((2,2), strides=2),\n",
    "                Flatten(),\n",
    "                Dense(4, activation='softmax')\n",
    "            ]\n",
    "        )\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss = 'categorical_crossentropy',\n",
    "            metrics = ['accuracy']\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 13:12:33.078412: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/charles/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-01-13 13:12:33.078449: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-13 13:12:33.078482: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (charles-HP-EliteBook-Folio-9480m): /proc/driver/nvidia/version does not exist\n",
      "2023-01-13 13:12:33.078909: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = fruits_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Functional)           (None, 3, 3, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 20,026,436\n",
      "Trainable params: 4,721,668\n",
      "Non-trainable params: 15,304,768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 1, verbose=1,factor=0.3, min_lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 02:24:21.752059: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "15/15 [==============================] - 67s 5s/step - loss: 1.2302 - accuracy: 0.5458 - val_loss: 0.5575 - val_accuracy: 0.7667\n",
      "Epoch 2/20\n",
      "15/15 [==============================] - 87s 6s/step - loss: 0.2996 - accuracy: 0.8708 - val_loss: 0.2289 - val_accuracy: 0.9167\n",
      "Epoch 3/20\n",
      "15/15 [==============================] - 72s 5s/step - loss: 0.1200 - accuracy: 0.9542 - val_loss: 0.2010 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005.\n",
      "Epoch 4/20\n",
      "15/15 [==============================] - 65s 4s/step - loss: 0.0583 - accuracy: 0.9792 - val_loss: 0.2168 - val_accuracy: 0.9167\n",
      "Epoch 5/20\n",
      "15/15 [==============================] - 68s 5s/step - loss: 0.0120 - accuracy: 0.9958 - val_loss: 0.1543 - val_accuracy: 0.9500\n",
      "Epoch 6/20\n",
      "15/15 [==============================] - 65s 4s/step - loss: 0.0086 - accuracy: 0.9958 - val_loss: 0.2056 - val_accuracy: 0.9167\n",
      "Epoch 7/20\n",
      "15/15 [==============================] - 64s 4s/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.1761 - val_accuracy: 0.9500\n",
      "Epoch 8/20\n",
      "15/15 [==============================] - 64s 4s/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1661 - val_accuracy: 0.9500\n",
      "Epoch 9/20\n",
      "15/15 [==============================] - 63s 4s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1683 - val_accuracy: 0.9500\n",
      "Epoch 10/20\n",
      "15/15 [==============================] - 64s 4s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1824 - val_accuracy: 0.9500\n",
      "Epoch 11/20\n",
      "15/15 [==============================] - 64s 4s/step - loss: 8.0369e-04 - accuracy: 1.0000 - val_loss: 0.1872 - val_accuracy: 0.9500\n",
      "Epoch 12/20\n",
      "15/15 [==============================] - 64s 4s/step - loss: 6.1043e-04 - accuracy: 1.0000 - val_loss: 0.1941 - val_accuracy: 0.9500\n",
      "Epoch 13/20\n",
      "15/15 [==============================] - 64s 4s/step - loss: 5.1605e-04 - accuracy: 1.0000 - val_loss: 0.1854 - val_accuracy: 0.9500\n",
      "Epoch 14/20\n",
      "15/15 [==============================] - 64s 4s/step - loss: 4.1259e-04 - accuracy: 1.0000 - val_loss: 0.1879 - val_accuracy: 0.9500\n",
      "Epoch 15/20\n",
      "15/15 [==============================] - 65s 4s/step - loss: 3.3228e-04 - accuracy: 1.0000 - val_loss: 0.1855 - val_accuracy: 0.9667\n",
      "Epoch 16/20\n",
      "15/15 [==============================] - 64s 4s/step - loss: 2.7163e-04 - accuracy: 1.0000 - val_loss: 0.1878 - val_accuracy: 0.9500\n",
      "Epoch 17/20\n",
      "15/15 [==============================] - 66s 4s/step - loss: 2.0752e-04 - accuracy: 1.0000 - val_loss: 0.1748 - val_accuracy: 0.9500\n",
      "Epoch 18/20\n",
      "15/15 [==============================] - 64s 4s/step - loss: 1.6951e-04 - accuracy: 1.0000 - val_loss: 0.1842 - val_accuracy: 0.9500\n",
      "Epoch 19/20\n",
      "15/15 [==============================] - 65s 4s/step - loss: 1.3551e-04 - accuracy: 1.0000 - val_loss: 0.1868 - val_accuracy: 0.9500\n",
      "Epoch 20/20\n",
      "15/15 [==============================] - 64s 4s/step - loss: 9.9689e-05 - accuracy: 1.0000 - val_loss: 0.1858 - val_accuracy: 0.9667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1756519460>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs = EPOCHS, validation_data = (X_test, y_test), callbacks = [learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../AI Models/fruits_classfication_20epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 6s 3s/step - loss: 0.1858 - accuracy: 0.9667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1857992261648178, 0.9666666388511658]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREDICTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(X_test), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 3, 0, 0, 0, 0, 1, 2, 1, 1, 1, 2, 1, 2, 1, 0, 3, 3, 0])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.argmax(y_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 3, 0, 0, 0, 0, 1, 3, 1, 1, 1, 2, 1, 2, 2, 0, 3, 3, 0, 3, 0,\n",
       "       0, 2, 3, 0, 3, 3, 0, 3, 3, 1, 3, 3, 1, 0, 0, 3, 1, 0, 0, 3, 1, 1,\n",
       "       3, 1, 1, 0, 1, 1, 1, 1, 3, 3, 0, 1, 2, 1, 3, 3])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18  0  0  0]\n",
      " [ 0 18  0  0]\n",
      " [ 0  1  4  0]\n",
      " [ 0  0  1 18]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        18\n",
      "           1       0.95      1.00      0.97        18\n",
      "           2       0.80      0.80      0.80         5\n",
      "           3       1.00      0.95      0.97        19\n",
      "\n",
      "    accuracy                           0.97        60\n",
      "   macro avg       0.94      0.94      0.94        60\n",
      "weighted avg       0.97      0.97      0.97        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.67 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"{accuracy_score(y, y_pred) * 100:0.2f} %\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICT ON UNSEEN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Unseen data\n",
    "unseen_dir = \"./unseen_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6496/407720218.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(data)\n"
     ]
    }
   ],
   "source": [
    "data = get_data(unseen_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_u, y_u = get_x_and_y(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_u = normalize(X_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_u, y_u = reshape_data(X_u, y_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 13:38:58.667167: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/charles/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-01-13 13:38:58.667266: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-13 13:38:58.667303: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (charles-HP-EliteBook-Folio-9480m): /proc/driver/nvidia/version does not exist\n",
      "2023-01-13 13:38:58.667702: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "fruit_model = load_model(\"../AI Models/fruits_classfication_20epochs.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Functional)           (None, 3, 3, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 20,026,436\n",
      "Trainable params: 4,721,668\n",
      "Non-trainable params: 15,304,768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fruit_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_unseen = np.argmax(fruit_model.predict(X_u), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 0, 0, 3, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.argmax(y_u, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y, y_pred_unseen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSION\n",
    "\n",
    "Trained model has a final accuracy of 96% on test data which is very good. However, it performs poorly on Unseen data with accuracy of 66.66%. The reason for this perfomance is because it is unable to defferentiate between mixed fruits and multiple fruits in the same picture. Thus, if we have more than one banana in the same photo without any other fruit, the model classifies it has mixed instead of banana.\n",
    "\n",
    "**What is solution to this issue?**\n",
    "\n",
    "I think the best way to solve the issue is to have more additional data with images of plural fruits (of same species) and and retrain the model.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------**THE END**-------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
